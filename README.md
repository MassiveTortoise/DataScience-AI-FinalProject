Hello Professor and everybody else!

Project: Toxic Detector

Description: This model we used is a fine-tuned version of the DistilBERT model to classify toxic comments.

This model is intended to be used for classifying toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslims. The following table shows an evaluation score for different identity groups. You can learn the specific meaning of these metrics here. Those metrics show how well a model performs for a specific group. The larger the number, the better.



We used the following Dataset: [datasets/SetFit/toxic_conversations_50k](https://huggingface.co/datasets/SetFit/toxic_conversations_50k)

Link to Model we used: [martin-ha/toxic-comment-model](https://huggingface.co/martin-ha/toxic-comment-model)

Link to the Presentation Slides: [canva.com](https://www.canva.com/design/DAGWysa7YRg/svtNbFvotFFNE0C5zNDxwA/view?utm_content=DAGWysa7YRg&utm_campaign=designshare&utm_medium=link&utm_source=editor)
